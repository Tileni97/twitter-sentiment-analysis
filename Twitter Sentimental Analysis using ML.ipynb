{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa189ef",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis using Natural Language Processing (NLP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fb558",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we delve into the fascinating world of sentiment analysis using Twitter data. Sentiment analysis, a subfield of Natural Language Processing (NLP), involves analyzing text data to determine the sentiment expressed, such as positive, negative, or neutral. By examining tweets from Twitter, we can extract valuable insights into public opinion on various topics, ranging from politics to products and brands.\n",
    "\n",
    "This project leverages machine learning techniques to classify tweets based on sentiment, helping us understand how people feel about specific topics in real time. Using a dataset of tweets, we apply preprocessing steps such as tokenization, stopword removal, and stemming, followed by training machine learning models to predict the sentiment of unseen tweets.\n",
    "\n",
    "The aim of this project is to showcase the application of NLP and machine learning to real-world social media data, providing a powerful tool for sentiment analysis in various industries, including marketing, customer service, and public relations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9bd3f9",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The primary objective of this project is to:\n",
    "\n",
    "1. **Understand and process social media data**: Extract and preprocess data from Twitter, focusing on cleaning and preparing the text for analysis.\n",
    "2. **Apply Natural Language Processing (NLP) techniques**: Utilize various NLP methods such as tokenization, stopword removal, and stemming to prepare the text for sentiment classification.\n",
    "3. **Train and evaluate machine learning models**: Implement machine learning algorithms to classify tweet sentiments into categories such as positive, negative, and neutral.\n",
    "4. **Deploy the sentiment analysis model**: Provide a practical demonstration of how sentiment analysis can be applied to real-time social media data.\n",
    "5. **Gain insights into public opinion**: Explore how sentiment analysis can help analyze large amounts of social media data to uncover trends and sentiments about specific topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46c864",
   "metadata": {},
   "source": [
    "## Tools and Libraries Used\n",
    "\n",
    "- **Python**: The primary programming language used for data analysis and machine learning tasks.\n",
    "- **NLTK (Natural Language Toolkit)**: A library for processing and analyzing human language data.\n",
    "- **Scikit-learn**: A machine learning library for training, testing, and evaluating various models.\n",
    "- **Pandas**: Used for data manipulation and processing, particularly for loading and working with the dataset.\n",
    "- **Matplotlib**: A library used for creating visualizations and plots to understand the distribution of sentiment.\n",
    "- **Seaborn**: A data visualization library used to enhance the visual appeal and clarity of plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb553ae8",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The dataset used in this project is the **Sentiment140** dataset, which contains 1.6 million tweets labeled with sentiment labels (positive, negative, and neutral). The dataset includes the following columns:\n",
    "\n",
    "- **target**: Sentiment label (0 = negative, 4 = positive)\n",
    "- **id**: Unique identifier for the tweet\n",
    "- **date**: Date and time of the tweet\n",
    "- **flag**: Unused field\n",
    "- **user**: Username of the Twitter account\n",
    "- **text**: The text content of the tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ff385",
   "metadata": {},
   "source": [
    "## Steps Involved\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Load the dataset and examine the structure.\n",
    "   - Clean the text data by removing unnecessary characters, stopwords, and non-alphanumeric symbols.\n",
    "   - Tokenize the text and apply stemming to reduce words to their root form.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**:\n",
    "   - Analyze the distribution of sentiments in the dataset.\n",
    "   - Visualize the frequency of positive, negative, and neutral sentiments.\n",
    "\n",
    "3. **Feature Extraction**:\n",
    "   - Convert the cleaned text into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "4. **Model Training**:\n",
    "   - Split the data into training and test sets.\n",
    "   - Train machine learning models such as Logistic Regression, Naive Bayes, or Random Forest.\n",
    "\n",
    "5. **Model Evaluation**:\n",
    "   - Evaluate the performance of the trained model using accuracy score and other metrics.\n",
    "   - Tune the model parameters for better performance.\n",
    "\n",
    "6. **Prediction and Visualization**:\n",
    "   - Make predictions on unseen data (new tweets).\n",
    "   - Visualize the predicted sentiments and analyze trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395e0d5",
   "metadata": {},
   "source": [
    " #### installing Kaggle Library and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba68acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\chang\\anaconda3\\lib\\site-packages (1.6.17)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\chang\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\chang\\anaconda3\\lib\\site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\chang\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\chang\\anaconda3\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chang\\anaconda3\\lib\\site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\chang\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\chang\\anaconda3\\lib\\site-packages (from kaggle) (1.26.16)\n",
      "Requirement already satisfied: bleach in c:\\users\\chang\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\chang\\anaconda3\\lib\\site-packages (from bleach->kaggle) (23.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\chang\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\chang\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chang\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chang\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\chang\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# installing Kaggle Library\n",
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74ddab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                              title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
      "---------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
      "stealthtechnologies/predict-student-performance-dataset          Predict Student Performance                          12KB  2024-12-26 12:57:04           1322         36  1.0              \n",
      "bhadramohit/customer-shopping-latest-trends-dataset              Customer Shopping (Latest Trends) Dataset            76KB  2024-11-23 15:26:12          21803        406  1.0              \n",
      "ankushpanday1/heart-attack-in-youth-of-india                     Heart attack in youth of India                      298KB  2025-01-02 15:20:31            831         26  1.0              \n",
      "oktayrdeki/heart-disease                                         Heart Disease                                       568KB  2024-12-29 13:26:49           1067         31  1.0              \n",
      "mhassansaboor/toyota-motors-stock-data-2980-2024                 Toyota Motors Stock Data (1980-2024)                189KB  2024-12-28 20:26:56            928         40  1.0              \n",
      "abdulmalik1518/the-ultimate-cars-dataset-2024                    The Ultimate Cars Dataset 2024                       25KB  2024-12-30 14:45:41            851         29  1.0              \n",
      "hopesb/student-depression-dataset                                Student Depression Dataset.                         454KB  2024-11-22 17:56:03          16992        245  1.0              \n",
      "fatmanur12/new-york-air-quality                                  New York Air Quality                                166KB  2025-01-01 19:19:59            628         26  1.0              \n",
      "govindaramsriram/sleep-time-prediction                           Sleep Time Prediction                                28KB  2024-12-28 17:08:56           1358         31  1.0              \n",
      "arifmia/agricultural-land-suitability-and-soil-quality           Agricultural Land Suitability and Soil Quality       27KB  2025-01-02 16:57:03            686         21  0.7058824        \n",
      "b'denkuznetz/food-delivery-time-prediction                         Food Delivery Time Prediction \\xf0\\x9f\\x9b\\xb5                      12KB  2024-12-23 13:12:14           1486         40  1.0              '\n",
      "pooriamst/online-shopping                                        Customer Satisfaction Response to AI                  7KB  2025-01-02 06:16:56            616         22  0.88235295       \n",
      "taimoor888/top-100-youtube-channels-in-2024                      Top 100 YouTube Channels in 2024                      3KB  2024-12-15 16:09:25           1518         43  1.0              \n",
      "govindaramsriram/crop-yield-of-a-farm                            Crop Yield of a Farm                                 28KB  2024-12-28 18:12:16            891         29  1.0              \n",
      "oktayrdeki/houses-in-london                                      Houses in London                                     21KB  2024-12-15 19:27:42           1717         39  1.0              \n",
      "mujtabamatin/air-quality-and-pollution-assessment                Air Quality and Pollution Assessment                 84KB  2024-12-04 15:29:51           7972        119  1.0              \n",
      "yaaryiitturan/global-tech-salary-dataset                         Global Tech Salary Dataset                           43KB  2024-12-30 12:52:44            629         28  1.0              \n",
      "hassanelfattmi/why-do-customers-leave-can-you-spot-the-churners  Why Do Customers Leave? Can You Spot the Churners?    3MB  2024-12-21 11:49:00            752         21  1.0              \n",
      "mhassansaboor/intel-stock-data-1980-2024                         Intel Stock Data (1980-2024)                        281KB  2024-12-25 16:12:36            779         30  1.0              \n",
      "willianoliveiragibin/water-sanitation-and-hygiene                Water, Sanitation and Hygiene                        33KB  2024-12-30 20:41:52            404         23  1.0              \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory containing `kaggle.json`\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = r\"C:\\Users\\Chang\\Downloads\"\n",
    "\n",
    "# Test the Kaggle API\n",
    "!kaggle datasets list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77da1fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
      "License(s): other\n",
      "sentiment140.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download the Sentiment140 dataset\n",
    "!kaggle datasets download -d kazanova/sentiment140\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7689b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Unzip the downloaded file\n",
    "with zipfile.ZipFile(\"sentiment140.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"sentiment140_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a46a9",
   "metadata": {},
   "source": [
    "#### Importing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684bc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation and analysis\n",
    "import pandas as pd  # Used for data manipulation and analysis, especially for working with DataFrames\n",
    "import numpy as np  # Provides support for numerical computations, arrays, and mathematical operations\n",
    "\n",
    "# Import libraries for text preprocessing\n",
    "import re  # Regular expressions, used for cleaning and searching text patterns\n",
    "from nltk.corpus import stopwords  # Provides a list of common stopwords to remove from text (e.g., \"and\", \"the\")\n",
    "from nltk.stem.porter import PorterStemmer  # Used for stemming, which reduces words to their root forms (e.g., \"running\" -> \"run\")\n",
    "\n",
    "# Import libraries for feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Converts text into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "# Import libraries for model training and evaluation\n",
    "from sklearn.model_selection import train_test_split  # Splits the dataset into training and testing sets\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression algorithm for classification tasks\n",
    "from sklearn.metrics import accuracy_score  # Calculates the accuracy of the model's predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f914d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary NLTK module for stopwords\n",
    "import nltk  # Natural Language Toolkit, a library for processing and analyzing human language data\n",
    "\n",
    "# Download the stopwords data from NLTK (this step is needed only once)\n",
    "nltk.download('stopwords')  # Downloads the list of stopwords in various languages from NLTK\n",
    "\n",
    "# Printing the stopwords in English\n",
    "print(stopwords.words('english'))  # Displays the list of common stopwords in English (e.g., \"the\", \"and\", \"is\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdea089",
   "metadata": {},
   "source": [
    "# 1. **Data Preprocessing**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be4e8907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599999, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the CSV file to a pandas DataFrame\n",
    "twitter_data = pd.read_csv(r\"C:\\Users\\Chang\\Downloads\\sentiment140_dataset\\training.1600000.processed.noemoticon.csv\", encoding='latin-1')\n",
    "\n",
    "# Checking the number of rows and columns\n",
    "print(twitter_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "735da987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the first 5 rows of the dataframe\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d5ea935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the column names\n",
    "column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# Load the data again with the new column names\n",
    "twitter_data = pd.read_csv(r\"C:\\Users\\Chang\\Downloads\\sentiment140_dataset\\training.1600000.processed.noemoticon.csv\", \n",
    "                           encoding='latin-1', names=column_names, header=None)\n",
    "\n",
    "# Display the first few rows of the dataset with new column names\n",
    "twitter_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd099752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "id        0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there are any missing values in the dataset\n",
    "twitter_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e46ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    800000\n",
      "4    800000\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking the distribution of the 'target' column\n",
    "# The 'target' column contains the sentiment labels: 0 for negative and 4 for positive sentiment.\n",
    "# By using 'value_counts()', we can check how many negative (0) and positive (4) sentiments are present in the dataset.\n",
    "# This gives us an idea of the class distribution, which helps us understand if the dataset is balanced or not.\n",
    "\n",
    "print(twitter_data['target'].value_counts())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e04c5",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "twitter_data['target'].value_counts(): This line counts how many occurrences of each label (0 and 4) are present in the 'target' column. We see that both labels (0 and 4) have equal occurrences, making the dataset balanced in terms of sentiment labels (negative vs. positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8171184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    800000\n",
      "1    800000\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert the target value \"4\" to \"1\" to make the dataset more standard\n",
    "# In machine learning tasks, it's common to represent the target sentiment as binary: \n",
    "# 0 for negative and 1 for positive. \n",
    "# This step ensures that we have a binary classification problem instead of a multi-class classification.\n",
    "\n",
    "twitter_data.replace({'target': {4: 1}}, inplace=True)\n",
    "\n",
    "# Checking the distribution of the target after conversion\n",
    "# We now expect to see '0' for negative sentiment and '1' for positive sentiment in the 'target' column.\n",
    "\n",
    "target_distribution = twitter_data['target'].value_counts()\n",
    "\n",
    "# Output the distribution\n",
    "print(target_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb20a90",
   "metadata": {},
   "source": [
    "### Importance of Checking the Target Column Distribution\n",
    "\n",
    "The **target column** represents the sentiment of the tweets, where:\n",
    "- **0** indicates a **negative sentiment**\n",
    "- **1** indicates a **positive sentiment**\n",
    "\n",
    "By running `value_counts()`, we can quickly observe the distribution of sentiment labels in the dataset. This is important for several reasons:\n",
    "1. **Class Imbalance**: If the dataset contains a significantly higher number of one class (e.g., positive or negative), it might affect the performance of machine learning models. In this case, the dataset is balanced with an equal number of positive (1) and negative (0) sentiments, which is ideal for training a model without bias.\n",
    "2. **Model Evaluation**: Knowing the distribution helps us better evaluate the performance of our sentiment analysis model, as class imbalance can lead to misleading accuracy scores. In this case, since we have a balanced dataset, our model will be less likely to be biased towards predicting one sentiment over the other.\n",
    "3. **Data Insights**: This step helps us get a quick overview of the dataset and ensures there are no unexpected anomalies, such as missing data or imbalanced labels.\n",
    "\n",
    "In this particular dataset, we can see:\n",
    "- There are **800,000** negative tweets (label 0).\n",
    "- There are **800,000** positive tweets (label 1).\n",
    "\n",
    "This is an evenly distributed dataset, making it suitable for training a model without the risk of bias toward one sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347b5db",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is a crucial step in Natural Language Processing (NLP) that reduces words to their root form. This process ensures that variations of a word, such as \"running,\" \"runner,\" and \"runs,\" are treated as the same, which helps in:\n",
    "\n",
    "** Text Normalization: Ensures consistency by reducing words to their base form.\n",
    "** Reducing Vocabulary Size: Groups similar words, making computations faster and easier.\n",
    "** Improving Model Accuracy: Focuses on meaning rather than grammatical variations.\n",
    "** Relevance: In sentiment analysis, stemming helps simplify tweets that often contain informal and varied language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd7ddb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Porter Stemmer\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2cc52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stemming function\n",
    "def stemming(content):\n",
    "    \"\"\"\n",
    "    Function to preprocess and stem a given text.\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove non-alphabetic characters using regex.\n",
    "    2. Convert the text to lowercase for consistency.\n",
    "    3. Split the text into individual words (tokens).\n",
    "    4. Remove stopwords (common words like 'the', 'and', etc.).\n",
    "    5. Stem each word to its root form.\n",
    "    6. Join the stemmed words back into a single string.\n",
    "    \n",
    "    Parameters:\n",
    "    content (str): A string of text to preprocess and stem.\n",
    "    \n",
    "    Returns:\n",
    "    str: The preprocessed and stemmed text.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove non-alphabetic characters\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    \n",
    "    # Step 2: Convert text to lowercase\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    \n",
    "    # Step 3: Split the text into individual words (tokens)\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    \n",
    "    # Step 4: Remove stopwords and apply stemming\n",
    "    stemmed_content = [\n",
    "        port_stem.stem(word) for word in stemmed_content if word not in stopwords.words('english')\n",
    "    ]\n",
    "    \n",
    "    # Step 5: Join the stemmed words back into a single string\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    \n",
    "    return stemmed_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "487f6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the stemming function to the 'text' column\n",
    "# Takes about 50 minutes to complete this execution\n",
    "twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99b4f0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>switchfoot http twitpic com zl awww bummer sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset updat facebook text might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass behav mad see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     stemmed_content  \n",
       "0  switchfoot http twitpic com zl awww bummer sho...  \n",
       "1  upset updat facebook text might cri result sch...  \n",
       "2  kenichan dive mani time ball manag save rest g...  \n",
       "3                    whole bodi feel itchi like fire  \n",
       "4                      nationwideclass behav mad see  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The new column stemmed_content contains the processed text from the text \n",
    "column, where all words are reduced to their root forms through stemming, \n",
    "helping standardize the data for analysis.\n",
    "'''\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a0d240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          switchfoot http twitpic com zl awww bummer sho...\n",
      "1          upset updat facebook text might cri result sch...\n",
      "2          kenichan dive mani time ball manag save rest g...\n",
      "3                            whole bodi feel itchi like fire\n",
      "4                              nationwideclass behav mad see\n",
      "                                 ...                        \n",
      "1599995                           woke school best feel ever\n",
      "1599996    thewdb com cool hear old walt interview http b...\n",
      "1599997                         readi mojo makeov ask detail\n",
      "1599998    happi th birthday boo alll time tupac amaru sh...\n",
      "1599999    happi charitytuesday thenspcc sparkschar speak...\n",
      "Name: stemmed_content, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# For the sentiment analysis task, we are focusing on two main columns:\n",
    "# 1. 'stemmed_content': Contains the processed text data with stemming applied (reducing words to their root form).\n",
    "\n",
    "print(twitter_data['stemmed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7a06db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "          ..\n",
      "1599995    1\n",
      "1599996    1\n",
      "1599997    1\n",
      "1599998    1\n",
      "1599999    1\n",
      "Name: target, Length: 1600000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. 'target': Contains the sentiment labels, where 0 indicates negative sentiment and 1 indicates positive sentiment.\n",
    "print(twitter_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2a8b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the data (features) and the label (target)\n",
    "X = twitter_data['stemmed_content'].values  # X contains the preprocessed text (features)\n",
    "Y = twitter_data['target'].values           # Y contains the sentiment labels (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2174624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['switchfoot http twitpic com zl awww bummer shoulda got david carr third day'\n",
      " 'upset updat facebook text might cri result school today also blah'\n",
      " 'kenichan dive mani time ball manag save rest go bound' ...\n",
      " 'readi mojo makeov ask detail'\n",
      " 'happi th birthday boo alll time tupac amaru shakur'\n",
      " 'happi charitytuesday thenspcc sparkschar speakinguph h']\n"
     ]
    }
   ],
   "source": [
    "# The output displays the first 3 and last 3 tweets from the dataset, which contains over a million tweets.\n",
    "# This is a small sample from the `X` (features) array, showing how the text data looks after preprocessing (stemming).\n",
    "# Due to the large size of the dataset, we only display a snippet here for clarity.\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36b4fe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# The output displays a small sample of the target labels from the dataset (Y).\n",
    "# Each value represents the sentiment of the corresponding tweet: 0 for negative sentiment and 1 for positive sentiment.\n",
    "# Due to the large size of the dataset, we are only displaying a snippet of the labels for clarity.\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be4737",
   "metadata": {},
   "source": [
    "### Splitting the Data into Training and Test Sets\n",
    "In this step, we will split the dataset into two parts:\n",
    "\n",
    "Training Data: This subset will be used to train our machine learning model. The model will learn patterns from this data to make predictions.\n",
    "\n",
    "Test Data: This subset will be used to evaluate the performance of the trained model. By testing the model on data it hasn't seen before, we can assess how well it generalizes to new, unseen data.\n",
    "\n",
    "This separation helps us ensure that the model is not just memorizing the data (overfitting) but is able to make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e7b64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X,               # Feature data (stemmed tweets)\n",
    "    Y,               # Target labels (sentiment labels)\n",
    "    test_size=0.2,   # 20% of the data will be used for testing\n",
    "    stratify=Y,      # Ensures the target labels are evenly distributed in both train and test sets\n",
    "    random_state=2   # Seed for reproducibility of the split\n",
    ")\n",
    "\n",
    "# Now we have:\n",
    "# X_train, Y_train: Data for training the model\n",
    "# X_test, Y_test: Data for evaluating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdc975b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000,) (1280000,) (320000,)\n"
     ]
    }
   ],
   "source": [
    "# Printing the shapes of the original, training, and test datasets\n",
    "print(X.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc721ea",
   "metadata": {},
   "source": [
    "### Explanation of Output:\n",
    "\n",
    "- **`X.shape: (1600000,)`**: The original dataset contains 1,600,000 tweets.\n",
    "- **`X_train.shape: (1280000,)`**: 1,280,000 tweets (80% of the data) are allocated to the training set.\n",
    "- **`X_test.shape: (320000,)`**: 320,000 tweets (20% of the data) are allocated to the test set.\n",
    "\n",
    "The dataset has been correctly split into 80% for training and 20% for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a2224",
   "metadata": {},
   "source": [
    "### Converting Textual Data to Numerical Data\n",
    "\n",
    "In this step, we convert the textual data into numerical data that can be processed by machine learning models. Since the model cannot understand text directly, we need to transform the text (tweets) into a numerical format.\n",
    "\n",
    "We'll use **TF-IDF (Term Frequency - Inverse Document Frequency)**, which is a technique to evaluate the importance of a word in a document relative to the entire dataset. This helps in capturing the significance of each word in a tweet.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a0ed6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the TfidfVectorizer, which will convert text to numerical data\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it into numerical data\n",
    "# The fit_transform method both learns the vocabulary and transforms the text to TF-IDF features\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the already learned vocabulary\n",
    "# The transform method only applies the transformation without altering the vocabulary\n",
    "X_test = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0a94871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 443066)\t0.4484755317023172\n",
      "  (0, 235045)\t0.41996827700291095\n",
      "  (0, 109306)\t0.3753708587402299\n",
      "  (0, 185193)\t0.5277679060576009\n",
      "  (0, 354543)\t0.3588091611460021\n",
      "  (0, 436713)\t0.27259876264838384\n",
      "  (1, 160636)\t1.0\n",
      "  (2, 288470)\t0.16786949597862733\n",
      "  (2, 132311)\t0.2028971570399794\n",
      "  (2, 150715)\t0.18803850583207948\n",
      "  (2, 178061)\t0.1619010109445149\n",
      "  (2, 409143)\t0.15169282335109835\n",
      "  (2, 266729)\t0.24123230668976975\n",
      "  (2, 443430)\t0.3348599670252845\n",
      "  (2, 77929)\t0.31284080750346344\n",
      "  (2, 433560)\t0.3296595898028565\n",
      "  (2, 406399)\t0.32105459490875526\n",
      "  (2, 129411)\t0.29074192727957143\n",
      "  (2, 407301)\t0.18709338684973031\n",
      "  (2, 124484)\t0.1892155960801415\n",
      "  (2, 109306)\t0.4591176413728317\n",
      "  (3, 172421)\t0.37464146922154384\n",
      "  (3, 411528)\t0.27089772444087873\n",
      "  (3, 388626)\t0.3940776331458846\n",
      "  (3, 56476)\t0.5200465453608686\n",
      "  :\t:\n",
      "  (1279996, 390130)\t0.22064742191076112\n",
      "  (1279996, 434014)\t0.2718945052332447\n",
      "  (1279996, 318303)\t0.21254698865277746\n",
      "  (1279996, 237899)\t0.2236567560099234\n",
      "  (1279996, 291078)\t0.17981734369155505\n",
      "  (1279996, 412553)\t0.18967045002348676\n",
      "  (1279997, 112591)\t0.7574829183045267\n",
      "  (1279997, 273084)\t0.4353549002982409\n",
      "  (1279997, 5685)\t0.48650358607431304\n",
      "  (1279998, 385313)\t0.4103285865588191\n",
      "  (1279998, 275288)\t0.38703346602729577\n",
      "  (1279998, 162047)\t0.34691726958159064\n",
      "  (1279998, 156297)\t0.3137096161546449\n",
      "  (1279998, 153281)\t0.28378968751027456\n",
      "  (1279998, 435463)\t0.2851807874350361\n",
      "  (1279998, 124765)\t0.32241752985927996\n",
      "  (1279998, 169461)\t0.2659980990397061\n",
      "  (1279998, 93795)\t0.21717768937055476\n",
      "  (1279998, 412553)\t0.2816582375021589\n",
      "  (1279999, 96224)\t0.5416162421321443\n",
      "  (1279999, 135384)\t0.6130934129868719\n",
      "  (1279999, 433612)\t0.3607341026233411\n",
      "  (1279999, 435572)\t0.31691096877786484\n",
      "  (1279999, 31410)\t0.248792678366695\n",
      "  (1279999, 242268)\t0.19572649660865402\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c1c4e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 420984)\t0.17915624523539803\n",
      "  (0, 409143)\t0.31430470598079707\n",
      "  (0, 398906)\t0.3491043873264267\n",
      "  (0, 388348)\t0.21985076072061738\n",
      "  (0, 279082)\t0.1782518010910344\n",
      "  (0, 271016)\t0.4535662391658828\n",
      "  (0, 171378)\t0.2805816206356073\n",
      "  (0, 138164)\t0.23688292264071403\n",
      "  (0, 132364)\t0.25525488955578596\n",
      "  (0, 106069)\t0.3655545001090455\n",
      "  (0, 67828)\t0.26800375270827315\n",
      "  (0, 31168)\t0.16247724180521766\n",
      "  (0, 15110)\t0.1719352837797837\n",
      "  (1, 366203)\t0.24595562404108307\n",
      "  (1, 348135)\t0.4739279595416274\n",
      "  (1, 256777)\t0.28751585696559306\n",
      "  (1, 217562)\t0.40288153995289894\n",
      "  (1, 145393)\t0.575262969264869\n",
      "  (1, 15110)\t0.211037449588008\n",
      "  (1, 6463)\t0.30733520460524466\n",
      "  (2, 400621)\t0.4317732461913093\n",
      "  (2, 256834)\t0.2564939661498776\n",
      "  (2, 183312)\t0.5892069252021465\n",
      "  (2, 89448)\t0.36340369428387626\n",
      "  (2, 34401)\t0.37916255084357414\n",
      "  :\t:\n",
      "  (319994, 123278)\t0.4530341382559843\n",
      "  (319995, 444934)\t0.3211092817599261\n",
      "  (319995, 420984)\t0.22631428606830145\n",
      "  (319995, 416257)\t0.23816465111736276\n",
      "  (319995, 324496)\t0.3613167933647574\n",
      "  (319995, 315813)\t0.28482299145634127\n",
      "  (319995, 296662)\t0.39924856793840147\n",
      "  (319995, 232891)\t0.25741278545890767\n",
      "  (319995, 213324)\t0.2683969144317078\n",
      "  (319995, 155493)\t0.2770682832971668\n",
      "  (319995, 109379)\t0.30208964848908326\n",
      "  (319995, 107868)\t0.3339934973754696\n",
      "  (319996, 438709)\t0.4143006291901984\n",
      "  (319996, 397506)\t0.9101400928717545\n",
      "  (319997, 444770)\t0.2668297951055569\n",
      "  (319997, 416695)\t0.29458327588067873\n",
      "  (319997, 349904)\t0.32484594100566083\n",
      "  (319997, 288421)\t0.48498483387153407\n",
      "  (319997, 261286)\t0.37323893626855326\n",
      "  (319997, 169411)\t0.403381646999604\n",
      "  (319997, 98792)\t0.4463892055808332\n",
      "  (319998, 438748)\t0.719789181620468\n",
      "  (319998, 130192)\t0.6941927210956169\n",
      "  (319999, 400636)\t0.2874420848216212\n",
      "  (319999, 389755)\t0.9577980203954275\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d78cf3",
   "metadata": {},
   "source": [
    "##### Explanation of the Output:\n",
    "\n",
    "The output represents the **numerical transformation** of the textual data using `TfidfVectorizer`. \n",
    "\n",
    "- Each row corresponds to a specific tweet in the dataset.\n",
    "- Each column represents a unique word (or token) in the entire vocabulary of the dataset.\n",
    "- The values are the **TF-IDF scores**, which indicate the importance of a word in a specific tweet relative to its frequency across all tweets.\n",
    "\n",
    "This transformation converts textual data into a sparse matrix format, where most values are zero, optimizing memory usage. The numerical data is now ready for use in training machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f3dd5d",
   "metadata": {},
   "source": [
    "# Training the Machine Learning Model\n",
    "\n",
    "In this section, we will train a machine learning model to classify tweets into their respective sentiment categories (e.g., positive or negative). \n",
    "\n",
    "The training process involves:\n",
    "\n",
    "1. **Choosing a Model**: Selecting an appropriate machine learning algorithm for sentiment analysis.\n",
    "2. **Feeding the Data**: Using the numerical data (`X_train` and `Y_train`) to train the model to identify patterns and relationships between features (tweets) and their corresponding labels (sentiments).\n",
    "3. **Evaluation**: After training, the model will be tested on unseen data (`X_test`) to evaluate its accuracy and ability to generalize.\n",
    "\n",
    "The goal is to develop a robust model that can accurately predict the sentiment of new, unseen tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fabf01",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a simple and effective algorithm for binary classification tasks, such as sentiment analysis. It predicts probabilities using a sigmoid function and classifies data into categories based on a threshold (e.g., positive or negative sentiment). Its simplicity makes it a great starting point for building classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea54d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Logistic Regression model with a maximum iteration of 1000\n",
    "# max_iter specifies the maximum number of iterations the solver will take to converge\n",
    "model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78ef43b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Logistic Regression model using the training data\n",
    "# X_train contains the feature vectors for the training set\n",
    "# Y_train contains the corresponding labels for the training set\n",
    "model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec1d2a1",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Model evaluation is the process of assessing how well a trained machine learning model performs on unseen data. It helps determine whether the model is making accurate predictions and identifies potential overfitting or underfitting issues. Common evaluation metrics include accuracy, precision, recall, and F1-score.\n",
    "\n",
    "In this step, we'll test our model on the test dataset and calculate relevant metrics to measure its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22eb9e",
   "metadata": {},
   "source": [
    "#### Accuracy Score\n",
    "\n",
    "The accuracy score is a metric used to evaluate the performance of a classification model. It calculates the proportion of correct predictions (both true positives and true negatives) made by the model out of all predictions. \n",
    "A higher accuracy indicates that the model is performing well, though it may not always be the best metric if the dataset is imbalanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7918d0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Accuracy: 0.81023125\n"
     ]
    }
   ],
   "source": [
    "# Predicting labels for the training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "\n",
    "# Calculating accuracy score for the training data\n",
    "training_data_accuracy = accuracy_score(Y_train, X_train_prediction)\n",
    "\n",
    "# Printing the training data accuracy\n",
    "print(f\"Training Data Accuracy: {training_data_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c5ae4",
   "metadata": {},
   "source": [
    "#### Explanation of Output:\n",
    "\n",
    "The **Training Data Accuracy** is calculated as `0.81023125`, which means the model correctly predicted the labels for approximately **81.02%** of the training data. This is the proportion of correct predictions made by the model on the training set.\n",
    "\n",
    "A higher accuracy score generally indicates a better-performing model. However, it's important to evaluate the model on the test data as well to ensure that it generalizes well to unseen data and isn't overfitting to the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe64d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Accuracy: 0.778\n"
     ]
    }
   ],
   "source": [
    "# Predicting labels for the test data\n",
    "X_test_prediction = model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy score for the test data\n",
    "test_data_accuracy = accuracy_score(Y_test, X_test_prediction)\n",
    "\n",
    "# Printing the test data accuracy\n",
    "print(f\"Test Data Accuracy: {test_data_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50cab2",
   "metadata": {},
   "source": [
    "#### Test Data Accuracy\n",
    "\n",
    "The test data accuracy is `0.778`, which means the model correctly predicted the labels for 77.8% of the test data.\n",
    "\n",
    "#### Comparison with Training Data Accuracy\n",
    "\n",
    "- **Training Data Accuracy**: `0.810` (81%)\n",
    "- **Test Data Accuracy**: `0.778` (77.8%)\n",
    "\n",
    "The model performs well on both the training and test data, with only a small difference in accuracy (around 3.2%). This suggests that the model is not overfitting or underfitting. In other words, it generalizes well to unseen data.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "- **Good Model Fit**: The model shows a good balance between performance on training data and test data, which indicates that the model is likely not overfitting or underfitting.\n",
    "- **Overfitting or Underfitting**: If the training accuracy was much higher than the test accuracy, it could indicate overfitting. Conversely, if both accuracies were low, the model could be underfitting. In this case, the results suggest a well-fitted model.\n",
    "\n",
    "This is a positive outcome, but further optimization and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ee06a",
   "metadata": {},
   "source": [
    "### Saving the Trained Model\n",
    "\n",
    "Once we have trained our machine learning model, it is important to save it so that we can use it later for predictions without having to retrain the model every time. This can be achieved using the `pickle` module in Python.\n",
    "\n",
    "Pickle allows us to serialize the model into a file, which we can later load back into memory when needed. This is useful for deploying the model or for future use in applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "735e88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51d124a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as trained_model.sav\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename where the model will be saved\n",
    "filename = 'trained_model.sav'\n",
    "\n",
    "# Save the trained model using pickle\n",
    "# The 'wb' mode is used to write the model as a binary file\n",
    "with open(filename, 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Print a confirmation message that the model has been saved successfully\n",
    "print(f\"Model saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f25da",
   "metadata": {},
   "source": [
    "#### Saving the Trained Model\n",
    "\n",
    "In machine learning, once a model is trained and evaluated, we can save it for future use, rather than retraining it each time. Saving the model allows us to:\n",
    "\n",
    "- **Avoid retraining**: Training machine learning models can be time-consuming, especially with large datasets. By saving the trained model, we can quickly load it whenever needed for new predictions.\n",
    "- **Reuse the model**: The saved model can be deployed in different environments or used across multiple projects.\n",
    "- **Share the model**: If the model is effective, it can be shared with other team members, collaborators, or stakeholders for their use.\n",
    "\n",
    "In this case, we have saved the trained model as `trained_model.sav`. We can now load this model in the future and make predictions without needing to retrain it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0107e",
   "metadata": {},
   "source": [
    "#### Using the Saved Model for New Predictions\n",
    "\n",
    "Once we have saved the trained model, we can load it again to make predictions on new, unseen data. Here’s how we can use the saved model for making predictions without retraining it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8b02440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved model from the file\n",
    "loaded_model = pickle.load(open(r\"C:\\Users\\Chang\\Downloads\\trained_model.sav\", 'rb'))\n",
    "\n",
    "# Now, the 'loaded_model' can be used to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1472b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 1\n",
      "Predicted label: [1]\n",
      "Positive Tweet\n"
     ]
    }
   ],
   "source": [
    "# Selecting a new data point (tweet) from the test set\n",
    "X_new = X_test[200]\n",
    "\n",
    "# Printing the true label for the selected test data point\n",
    "print(\"True label:\", Y_test[200])\n",
    "\n",
    "# Making a prediction on the selected data point\n",
    "prediction = model.predict(X_new)\n",
    "\n",
    "# Printing the predicted label\n",
    "print(\"Predicted label:\", prediction)\n",
    "\n",
    "# Checking if the prediction is 0 (negative) or 1 (positive)\n",
    "if prediction[0] == 0:\n",
    "    print('Negative Tweet')\n",
    "else:\n",
    "    print('Positive Tweet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5415d",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "We selected a tweet from the test set (X_test[200]), and its true sentiment label is 1, which indicates a positive tweet.\n",
    "The model also predicted the sentiment to be 1, so the prediction was correct.\n",
    "As a result, the tweet is classified as Positive Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b145df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0\n",
      "Predicted label: [0]\n",
      "Negative Tweet\n"
     ]
    }
   ],
   "source": [
    "# Selecting a new data point (tweet) from the test set\n",
    "X_new = X_test[3]\n",
    "\n",
    "# Printing the true label for the selected test data point\n",
    "print(\"True label:\", Y_test[3])\n",
    "\n",
    "# Making a prediction on the selected data point\n",
    "prediction = model.predict(X_new)\n",
    "\n",
    "# Printing the predicted label\n",
    "print(\"Predicted label:\", prediction)\n",
    "\n",
    "# Checking if the prediction is 0 (negative) or 1 (positive)\n",
    "if prediction[0] == 0:\n",
    "    print('Negative Tweet')\n",
    "else:\n",
    "    print('Positive Tweet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653cb4e6",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "We selected another tweet from the test set (X_test[3]), and its true sentiment label is 0, which indicates a negative tweet.\n",
    "The model correctly predicted the sentiment as 0, so the prediction was correct.\n",
    "As a result, the tweet is classified as a Negative Tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d9b79",
   "metadata": {},
   "source": [
    "Confusion Matrix & Classification Report\n",
    "To evaluate the model's performance more comprehensively, we’ll use the Confusion Matrix and Classification Report. This will help us understand how well the model is classifying each sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "adfb6882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[121436  38564]\n",
      " [ 32476 127524]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77    160000\n",
      "           1       0.77      0.80      0.78    160000\n",
      "\n",
      "    accuracy                           0.78    320000\n",
      "   macro avg       0.78      0.78      0.78    320000\n",
      "weighted avg       0.78      0.78      0.78    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Predicting labels for the test data\n",
    "X_test_prediction = model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_test, X_test_prediction)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(Y_test, X_test_prediction)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2cf852",
   "metadata": {},
   "source": [
    "\n",
    "- **True Negatives (121,436)**: The number of negative tweets that were correctly predicted as negative.\n",
    "- **False Positives (38,564)**: The number of negative tweets that were incorrectly predicted as positive.\n",
    "- **False Negatives (32,476)**: The number of positive tweets that were incorrectly predicted as negative.\n",
    "- **True Positives (127,524)**: The number of positive tweets that were correctly predicted as positive.\n",
    "\n",
    "### Classification Report\n",
    "\n",
    "The **Classification Report** provides more detailed metrics:\n",
    "\n",
    "- **Accuracy**: The model correctly predicted the sentiment of 78% of the tweets.\n",
    "  \n",
    "- **Precision**: This indicates how accurate the model is when predicting a particular sentiment. \n",
    "  - Precision for negative sentiment (`0`) is 79%, and for positive sentiment (`1`), it's 77%.\n",
    "\n",
    "- **Recall**: This shows how well the model is able to identify all instances of a particular sentiment.\n",
    "  - Recall for negative sentiment (`0`) is 76%, and for positive sentiment (`1`), it's 80%.\n",
    "\n",
    "- **F1-Score**: The harmonic mean of precision and recall. This gives a balanced measure of the model’s performance.\n",
    "  - The F1-score for negative sentiment (`0`) is 77%, and for positive sentiment (`1`), it’s 78%.\n",
    "\n",
    "- **Macro Average**: The average of precision, recall, and F1-score across both classes, giving equal weight to each class.\n",
    "  \n",
    "- **Weighted Average**: The average of precision, recall, and F1-score, taking the class distribution into account.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The model has achieved a **78% accuracy**, which is quite good. The balance between precision, recall, and F1-score for both classes indicates that the model is performing well without a significant bias towards either positive or negative sentiments. \n",
    "\n",
    "However, there is some room for improvement, particularly in improving recall for negative sentiment (`0`). Further tuning and exploration of more advanced models could improve these results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2738e",
   "metadata": {},
   "source": [
    "# Final Report: Twitter Sentiment Analysis Using NLP\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this project, we explored the use of Natural Language Processing (NLP) and machine learning techniques to perform sentiment analysis on a large dataset of tweets. The primary objective was to classify tweets into positive and negative sentiments, helping us understand public opinion on various topics. We leveraged machine learning models, such as Logistic Regression, to predict the sentiment of unseen tweets after preprocessing the text data.\n",
    "\n",
    "### Key Steps in the Process:\n",
    "1. **Data Preprocessing**: The dataset was cleaned by removing unnecessary characters, stopwords, and applying stemming to ensure the text data was in a usable format for machine learning.\n",
    "2. **Exploratory Data Analysis (EDA)**: We analyzed the distribution of sentiment labels (positive, negative) and visualized the sentiment distribution.\n",
    "3. **Feature Extraction**: The cleaned text was converted into numerical features using the TF-IDF method, allowing us to use the data in a machine learning model.\n",
    "4. **Model Training**: Various machine learning models were trained, including Logistic Regression. We split the data into training and test sets, evaluating performance on both.\n",
    "5. **Model Evaluation**: The trained model's performance was evaluated using accuracy score, precision, recall, F1-score, and the confusion matrix.\n",
    "6. **Prediction and Visualization**: After evaluating the model, predictions were made on unseen data, and the sentiment of tweets was predicted.\n",
    "\n",
    "## Results\n",
    "\n",
    "- **Accuracy**: The model achieved an overall accuracy of **78%** on the test data.\n",
    "- **Confusion Matrix**: The confusion matrix showed that the model was able to correctly predict the sentiment for a majority of tweets but had some misclassifications (false positives and false negatives).\n",
    "  \n",
    "  The confusion matrix for the model's predictions was: [[121436 38564] [ 32476 127524]]\n",
    "  \n",
    "- **Classification Report**: The report displayed metrics such as precision, recall, and F1-score for both positive and negative sentiments. The balanced F1-scores (78% for both classes) indicate that the model performed well without significant bias towards either class.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This sentiment analysis model has demonstrated the power of machine learning and NLP in understanding and processing real-time social media data. With an accuracy of 78%, the model is effective at classifying tweets into positive and negative sentiments. However, there is still room for improvement, particularly in recall for the negative sentiment class. Future steps could include:\n",
    "\n",
    "- **Model Tuning**: Tuning the model parameters and exploring more advanced algorithms like Random Forest, Naive Bayes, or deep learning models could further improve performance.\n",
    "- **Handling Class Imbalance**: Investigating methods to deal with potential class imbalance may improve the recall for both positive and negative classes.\n",
    "- **Real-Time Deployment**: The model could be integrated with a real-time Twitter feed to analyze the sentiment of incoming tweets.\n",
    "\n",
    "In conclusion, sentiment analysis has a wide range of applications in industries like marketing, public relations, and customer service. This project not only showcases the importance of data preprocessing and machine learning but also provides a solid foundation for building systems that can analyze large amounts of social media data to uncover public opinion on a variety of topics.\n",
    "\n",
    "--- \n",
    "\n",
    "With that, we have completed the sentiment analysis model, successfully classifying tweets and gaining insights from social media data.\n",
    "\n",
    "Thank you!!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
